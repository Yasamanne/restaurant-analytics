{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098c283-51ab-4204-944a-42b85ad62ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Yasaman Emami\"\n",
    "__email__ = ['emami.yasamann@gmail.com','yasaman.emami@sjsu.edu']\n",
    "__sid__ = \"015325557\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c1c8e-c8cc-4273-946b-3f1883520489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "## Creating Schema\n",
    "\n",
    "labels = [\n",
    "    ('ID', typ.IntegerType()),\n",
    "    ('Name', typ.StringType()),\n",
    "    ('Street_Address', typ.StringType()),\n",
    "    ('City', typ.StringType()),\n",
    "    ('State', typ.StringType()),\n",
    "    ('Zip_Code', typ.LongType()),\n",
    "    ('Latitude', typ.FloatType()),\n",
    "    ('Longitude', typ.FloatType()),\n",
    "    ('Junk', typ.StringType()),\n",
    "    ('Phone_Number', typ.LongType()),\n",
    "    ('ID+Unknown_Date', typ.StringType()),\n",
    "    ('Unknown_Date', typ.StringType()),\n",
    "    ('Score?', typ.IntegerType()),\n",
    "    ('Inspection_type', typ.StringType()),\n",
    "    ('Unique_inspection_id', typ.StringType()),\n",
    "    ('Complaint', typ.StringType()),\n",
    "    ('Risk_Level', typ.StringType()),\n",
    "]\n",
    "schema = typ.StructType([\n",
    "typ.StructField(e[0], e[1], False) for e in labels\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "## Reading Data\n",
    "r2016 = spark.read.csv(\n",
    "'dataset2016.tsv', header=False, schema=schema, sep=\"\\t\"\n",
    ")\n",
    "r2016.name = 'r2016'\n",
    "r2017 = spark.read.csv(\n",
    "'dataset2017.tsv', header=False, schema=schema, sep=\"\\t\"\n",
    ")\n",
    "r2017.name = 'r2017'\n",
    "r2018 = spark.read.csv(\n",
    "'dataset2018.tsv', header=False, schema=schema, sep=\"\\t\"\n",
    ")\n",
    "r2018.name = 'r2018'\n",
    "dfs = r2016.union(r2017).union(r2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70699f8c-f9c3-45c1-99a4-ac2435f89ff6",
   "metadata": {},
   "source": [
    "## Preprocessing data like drop duplicates fill nulls,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ccce1-9e1e-4266-b483-bd76cfc3425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_duplicate = dfs.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d9f98-4ab5-4c22-8ee5-4bcd06c95d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "#encode risk level column to 0 and 1(0 for nulls and 1 for High,Moderate,Low risk values)\n",
    "data_coded_level = data_no_duplicate.withColumn('risk_level_coded', \n",
    "                                            when(data_no_duplicate.Risk_Level == \"Low Risk\", \"1\") \\\n",
    "                                           .when(data_no_duplicate.Risk_Level == \"Moderate Risk\", \"1\") \\\n",
    "                                           .when(data_no_duplicate.Risk_Level == \"High Risk\", \"1\") \\\n",
    "                                           .when(data_no_duplicate.Risk_Level.isNull(), \"0\"))\n",
    "#adding 2 seperate column as year and month that gots generated from date column original dataframe\n",
    "df = data_coded_level.withColumn('year', substring('Unknown_Date', 1,4)).withColumn('month', substring('Unknown_Date', 6,2))\n",
    "\n",
    "#df1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6acc2e-abea-4d31-a945-fe6c050f3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill null values for column score\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "\n",
    "df_stats = df.select(\n",
    "    _mean(col('Score?')).alias('mean_score')\n",
    "   \n",
    "   ).collect()\n",
    "\n",
    "mean_number = df_stats[0]['mean_score']\n",
    "df = df.na.fill(mean_number, [\"Score?\"])\n",
    "\n",
    "df = df.na.fill(\"unknown\", [\"Inspection_type\"]).na.fill(\"unknown\", [\"Complaint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbfc50-09d2-4e35-9b27-2102f2626ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for null values\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed51f67-7e20-43a5-93a4-654d5451fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df67ea-32cc-4afa-b525-1c942a56a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing data type from string to integer for some numeric columns\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType,BooleanType,DateType\n",
    "df = df.withColumn(\"risk_level_coded\",col(\"risk_level_coded\").cast(IntegerType())) \\\n",
    "         .withColumn(\"year\",col(\"year\").cast(IntegerType())) \\\n",
    "         .withColumn(\"month\",col(\"month\").cast(IntegerType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27dc44-a27a-42ed-8d2f-4e2f6d1996ee",
   "metadata": {},
   "source": [
    "## Understanding data and checking distinct data values for few columns to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d7305-3d86-428a-94e3-e540505a8271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\"Score?\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643ae13a-f431-4d3e-a506-58936b9332fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\"Inspection_type\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8703d41-9612-48cd-a492-8df6c30f7263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\"Complaint\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0a1ee-b328-42e1-9eb2-027782126af4",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af751eb2-fb26-4153-a4d6-399038da98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing obvious unrelated columns from feature list like address, zipcode,...\n",
    "df = df.select([\"Name\", \"Score?\", \"Inspection_type\", \"Complaint\", \"risk_level_coded\", \"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0bdebe-ac9d-4db7-8808-8c21f9a6ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the frequencies of categorical column values\n",
    "n_cols = [\"Name\",\"Score?\", \"risk_level_coded\", \"year\", \"month\"]\n",
    "categorical_cols = [\n",
    "e for e in df.columns if e not in n_cols\n",
    "]\n",
    "categorical_rdd = df.select(categorical_cols) \\\n",
    ".rdd.map(lambda row: [e for e in row])\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    agg = categorical_rdd.groupBy(lambda row: row[i]) \\\n",
    "        .map(lambda row: (row[0], len(row[1])))\n",
    "    print(\n",
    "        col, sorted(agg.collect(), key=lambda el: el[1],\n",
    "        reverse=True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20491089-298c-4774-8b4b-ed17e2d20fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical features, run a Chi-square test to determine\n",
    "# if there are significant differences.\n",
    "import pyspark.mllib.linalg as ln\n",
    "import pyspark.mllib.stat as st\n",
    "\n",
    "for cat in categorical_cols[1:]:\n",
    "    agg = df.groupby('risk_level_coded') \\\n",
    "    .pivot(cat).count()\n",
    "    agg_rdd = agg.rdd.map(lambda row: (row[1:])) \\\n",
    "    .flatMap(lambda row: [0 if e == None else e for e in row]) \\\n",
    "    .collect()\n",
    "    row_length = len(agg.collect()[0]) - 1\n",
    "    agg = ln.Matrices.dense(row_length, 2, agg_rdd)\n",
    "    test = st.Statistics.chiSqTest(agg)\n",
    "    print(cat, round(test.pValue, 4))\n",
    "print(ln.Matrices.dense(3,2, [1,2,3,4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b40d5-b83f-48f2-90fb-ccb595ae2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = [\"Name\", \"Score?\", \"Inspection_type\", \"Complaint\", \"risk_level_coded\", \"year\", \"month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370dcc26-138e-4694-b6ff-a10f501df3af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check for nulls in the final df for feeding ML model\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb9085-062b-40eb-9024-87e98eecdf6d",
   "metadata": {},
   "source": [
    "## Using label encoder for two of the selected features (Complaint, Inspection_Type) which is string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621cf20-0f0b-4fbc-b5be-7fa391ab0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be able to feed in onehotencoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"Complaint\", outputCol=\"Complaint_encode\") \n",
    "df = indexer.fit(df).transform(df) \n",
    "indexer2 = StringIndexer(inputCol=\"Inspection_type\", outputCol=\"Inspection_type_encode\") \n",
    "df = indexer2.fit(df).transform(df)\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e487f52-a8be-433b-86db-930a1b8b7d12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Droping two columns of (Complaint, Inspection_type) which already is generated in numeric categories\n",
    "df = df.drop(\"Complaint\", \"Inspection_type\")\n",
    "\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2985be3-4939-4d47-b2f3-033dc559a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting values just out of curiousity for checking how they are scattered \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "dfp = df.toPandas()\n",
    "plt.scatter(dfp[\"Score?\"],dfp[\"risk_level_coded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addfa93b-dd89-4d33-aef7-c8fd14ff8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numeric values\n",
    "import pyspark.ml.feature as ft\n",
    "\n",
    "# create Transformer: onehotencoder for two columns(Complaint, Inspection_Type)\n",
    "encoder = ft.OneHotEncoder(\n",
    "    inputCol='Inspection_type_encode', outputCol='Inspection_type_vec'\n",
    ")\n",
    " \n",
    "encoder2 = ft.OneHotEncoder(\n",
    "    inputCol='Complaint_encode', outputCol='Complaint_type_vec'\n",
    ")\n",
    "\n",
    "featurs_col = ['Score?', 'Inspection_type_encode', 'Complaint_encode']\n",
    "\n",
    "# since we have 3 features try with score only, score and inspection_type and all three features\n",
    "inputCols = ['Score?','Inspection_type_vec', 'Complaint_type_vec']\n",
    "\n",
    "inputCols2 = ['Score?','Inspection_type_vec']\n",
    "\n",
    "inputCols3 = ['Score?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88376e01-c0bc-40e3-84fc-70df94571ea8",
   "metadata": {},
   "source": [
    "## Please change inputCols to inputCols and 3 to see the different PR and ROC value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16458b-173c-4b9a-9413-53273c0d8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single column with all the features collated together.\n",
    "featuresCreator = ft.VectorAssembler(\n",
    "inputCols=inputCols,\n",
    "outputCol='features'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8214d1-44d0-492f-8200-1da8fd13ac6c",
   "metadata": {},
   "source": [
    "## Creating ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0837f80-4c6d-4347-9270-7453359539f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an estimator\n",
    "import pyspark.ml.classification as cl\n",
    "\n",
    "logistic = cl.LogisticRegression(\n",
    "maxIter=100, regParam=0.01, labelCol='risk_level_coded'\n",
    ")\n",
    "# Creating a pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[encoder,encoder2, featuresCreator, logistic\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e46b5-b509-4e23-92ea-5bb93fda89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating train data from 2016 to end of march 2018\n",
    "train = df.filter((df.year == 2016) | (df.year == 2017) | ((df.year == 2018) & (df.month<5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe2715-bfb6-4a73-9e94-bb17d99cda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data from april 2018 to the end\n",
    "test = df.filter((df.year == 2018) & (df.month >= 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45468a53-bc03-4e67-9d17-32a87e980996",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3240767-5568-45dc-8b21-6911ff73d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = df.filter((df.year == 2018) & (df.month == 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c09fe2-04e8-41ca-8d8f-5570d4c5373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a84a96-e32d-4dbd-ba68-d68eacc159a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model = pipeline.fit(train)\n",
    "# estimation\n",
    "test_model = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3126b57-1e43-4ee6-bfb2-bb01d47d08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b4d29-ef19-4f5f-9d39-830b0d769134",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438cc31-ec80-45c9-8b18-a65cddb1f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance\n",
    "test_model.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6744a-ef8f-4b18-909d-3386e7b7f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluating\n",
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "rawPredictionCol='probability',\n",
    "labelCol='risk_level_coded'\n",
    ")\n",
    "print(evaluator.evaluate(\n",
    "test_model, {evaluator.metricName: 'areaUnderROC'}\n",
    "))\n",
    "print(evaluator.evaluate(\n",
    "test_model, {evaluator.metricName: 'areaUnderPR'}\n",
    "))\n",
    "\n",
    "## =================== ##\n",
    "## if the selected features are inputCols2 ['Score?','Inspection_type_vec']\n",
    "## areaUnderROC ==> 0.9687605681033745\n",
    "## areaUnderPR ==> 0.9901315409221931\n",
    "\n",
    "\n",
    "## if if the selected features are inputCols3 ['Score?']\n",
    "## areaUnderROC ==> 0.493075464722042\n",
    "## areaUnderPR ==> 0.8225093580554805\n",
    "\n",
    "# so the pefrect classification model would be with the three features of ['Score?','Inspection_type_vec', 'Complaint_type_vec'] as\n",
    "# inputCols which has the value of 1 for both areaUnderROC and areaUnderPR which shows how perfect the model works building model with those features\n",
    "\n",
    "## ===================== ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802238df-2df6-4426-93a8-53ead9a2e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the Pipeline definition for later use\n",
    "pipelinePath = './restaurant_oneHotEncoder_Logistic_Pipeline'\n",
    "pipeline.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c298d-8082-4d7b-a069-8a5ba630f83d",
   "metadata": {},
   "source": [
    "## Predicting the restaurants which are likely to have another food safety issue in September 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef5319-33e1-4451-8271-e0f582b6ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "from pyspark.ml import PipelineModel\n",
    "modelPath = './restaurant_oneHotEncoder_Logistic_PipelineModel'\n",
    "model.write().overwrite().save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357d216-5e03-47ce-a4e7-5141e9d258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = model.transform(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96d0f41-bd94-4381-afa4-59ccb385edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df_predict.select([\"Name\", \"risk_level_coded\",\"prediction\" ]).filter(df_predict[\"prediction\"]==1)\n",
    "\n",
    "df_predict.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4eeba-ae64-4566-b991-e26b95da2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of restaunts which are likely to have another food safety issue in September 2018\n",
    "df_predict.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393703b-d408-4957-a854-a78cc9011d24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
